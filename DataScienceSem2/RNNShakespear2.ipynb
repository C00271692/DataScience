{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and prepare the dataset\n",
    "path_to_file = tf.keras.utils.get_file(\n",
    "    'shakespeare.txt', \n",
    "    'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
    ")\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n",
      "First 250 characters:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print length and preview\n",
    "print(f'Length of text: {len(text)} characters')\n",
    "print(f'First 250 characters:\\n{text[:250]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique characters: 65\n"
     ]
    }
   ],
   "source": [
    "# Process the text\n",
    "# Create vocabulary of unique characters\n",
    "vocab = sorted(set(text))\n",
    "print(f'Total unique characters: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping from character to index and vice versa\n",
    "char2index = {char: idx for idx, char in enumerate(vocab)}\n",
    "index2char = {idx: char for idx, char in enumerate(vocab)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text mapped to integers: [18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56]\n"
     ]
    }
   ],
   "source": [
    "# Convert text to numerical representation\n",
    "text_as_int = np.array([char2index[c] for c in text])\n",
    "print(f'Text mapped to integers: {text_as_int[:20]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training examples and targets\n",
    "seq_length = 100  # Length of sequence for a training example\n",
    "examples_per_epoch = len(text) // (seq_length + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742749198.671887   31063 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9682 MB memory:  -> device: 0, name: NVIDIA RTX 3500 Ada Generation Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: <_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# Create training batches\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape: {dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1536\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Add an InputLayer first to handle the batch size\n",
    "        tf.keras.layers.InputLayer(input_shape=(None,), batch_size=batch_size),\n",
    "        \n",
    "        # Embedding layer\n",
    "        tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim\n",
    "        ),\n",
    "        \n",
    "        # LSTM layer\n",
    "        tf.keras.layers.LSTM(\n",
    "            units=rnn_units,\n",
    "            return_sequences=True,\n",
    "            stateful=True,\n",
    "            recurrent_initializer=tf.keras.initializers.GlorotNormal()\n",
    "        ),\n",
    "        \n",
    "        # Dense output layer\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/Y4S2/DataScience/DataScienceSem2/.venv/lib64/python3.12/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">11,016,192</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">99,905</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │        \u001b[38;5;34m16,640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m)       │    \u001b[38;5;34m11,016,192\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)         │        \u001b[38;5;34m99,905\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,132,737</span> (42.47 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,132,737\u001b[0m (42.47 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,132,737</span> (42.47 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,132,737\u001b[0m (42.47 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and compile\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/172\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:55\u001b[0m 2s/step - loss: 4.1734"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742749200.854320   31336 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 31ms/step - loss: 2.9715\n",
      "Epoch 2/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 31ms/step - loss: 1.9046\n",
      "Epoch 3/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - loss: 1.6218\n",
      "Epoch 4/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 1.4831\n",
      "Epoch 5/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 1.3978\n",
      "Epoch 6/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 1.3375\n",
      "Epoch 7/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 1.2902\n",
      "Epoch 8/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 1.2499\n",
      "Epoch 9/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 1.2065\n",
      "Epoch 10/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 1.1655\n",
      "Epoch 11/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 1.1227\n",
      "Epoch 12/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - loss: 1.0761\n",
      "Epoch 13/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 1.0291\n",
      "Epoch 14/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 0.9756\n",
      "Epoch 15/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 0.9221\n",
      "Epoch 16/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 0.8651\n",
      "Epoch 17/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 0.8102\n",
      "Epoch 18/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 0.7503\n",
      "Epoch 19/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 0.6993\n",
      "Epoch 20/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 0.6474\n",
      "Epoch 21/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 0.6017\n",
      "Epoch 22/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 0.5599\n",
      "Epoch 23/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 0.5215\n",
      "Epoch 24/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 0.4890\n",
      "Epoch 25/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 76ms/step - loss: 0.4617\n",
      "Epoch 26/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - loss: 0.4361\n",
      "Epoch 27/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - loss: 0.4164\n",
      "Epoch 28/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - loss: 0.3986\n",
      "Epoch 29/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 0.3870\n",
      "Epoch 30/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 0.3720\n",
      "Epoch 31/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 0.3620\n",
      "Epoch 32/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 0.3524\n",
      "Epoch 33/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 0.3455\n",
      "Epoch 34/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 0.3395\n",
      "Epoch 35/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 0.3344\n",
      "Epoch 36/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 33ms/step - loss: 0.3288\n",
      "Epoch 37/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 43ms/step - loss: 0.3246\n",
      "Epoch 38/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 58ms/step - loss: 0.3223\n",
      "Epoch 39/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - loss: 0.3191\n",
      "Epoch 40/40\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 57ms/step - loss: 0.3148\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "EPOCHS = 40\n",
    "\n",
    "# Create a directory to save checkpoints\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Add .weights.h5 extension as required by Keras\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating Shakespeare text...\n",
      "\n",
      "Romeo's lines:\n",
      "ROMEO: My love for Juliet is befal.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "More hold that I said so? lay and beggary\n",
      "Then resolute as we are.\n",
      "\n",
      "GRUMIO:\n",
      "My cake is dough; but I'll in among these words,\n",
      "Infer favours shall be fearful.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "First, heaven be the worst that I should speak,\n",
      "Before I stay: but I have stay'd\n",
      "To tin thou shalt content that thou belike.'\n",
      "\n",
      "Second Citizen:\n",
      "Think you me all this whileld?\n",
      "\n",
      "Shepherd:\n",
      "Take hands, a bargain! and so doth he; and then\n",
      "Lady:\n",
      "Madam, I have one, and therein\n",
      "Of March wash y sweet saluteth me?\n",
      "Young son, it argues a thousand grains\n",
      "And her re usurp'd or fruit;\n",
      "Which was an adulteress;\n",
      "For to my rest that love what he would have strickenge-stand, the prince Frorth,\n",
      "Whose strange still, which will inform thee here.\n",
      "\n",
      "ROMEO:\n",
      "I have night's cloak:\n",
      "Who now the priest shout self-born in all:\n",
      "In all the throne names and undertakes,\n",
      "And I will take thy word: yet if thou swear'st,\n",
      "A thousand thanks, breed thee from the gods,\n",
      "Who preserves to save and by joy!\n",
      "\n",
      "TRANIO:\n",
      "Ay, marry, am I, s\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_string, num_generate=1000, temperature=1.0):\n",
    "    # Converting start string to numbers\n",
    "    input_indices = [char2index[c] for c in start_string]\n",
    "    \n",
    "    # Create a batch of identical inputs to match BATCH_SIZE\n",
    "    input_indices = tf.expand_dims(input_indices, 0)\n",
    "    input_indices = tf.repeat(input_indices, BATCH_SIZE, axis=0)\n",
    "    \n",
    "    # Empty string to store result\n",
    "    text_generated = []\n",
    "    \n",
    "    # Reset states manually by accessing the LSTM layer directly\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.LSTM):\n",
    "            layer.reset_states()\n",
    "    \n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_indices)\n",
    "        # Only use first sequence in batch for prediction\n",
    "        predictions = predictions[0]\n",
    "        \n",
    "        # Use temperature for sampling\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(tf.expand_dims(predictions[-1], 0), \n",
    "                                           num_samples=1)[-1, 0].numpy()\n",
    "        \n",
    "        # Add predicted character\n",
    "        text_generated.append(index2char[predicted_id])\n",
    "        \n",
    "        # Update input with new character for all sequences in batch\n",
    "        next_char = tf.expand_dims([predicted_id], 0)\n",
    "        next_char = tf.repeat(next_char, BATCH_SIZE, axis=0)\n",
    "        \n",
    "        # Shift input and add new character\n",
    "        if input_indices.shape[1] > 1:\n",
    "            input_indices = tf.concat([input_indices[:, 1:], next_char], axis=1)\n",
    "        else:\n",
    "            input_indices = next_char\n",
    "    \n",
    "    return start_string + ''.join(text_generated)\n",
    "\n",
    "# Generate text examples\n",
    "print(\"\\nGenerating Shakespeare text...\")\n",
    "print(\"\\nRomeo's lines:\")\n",
    "print(generate_text(model, start_string=\"ROMEO: My love for Juliet is \", temperature=1.0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hamlet's soliloquy: \n",
      "HAMLET: To be, or not to be part,\n",
      "Let me except the time to come.\n",
      "\n",
      "JULIET:\n",
      "O God, I have a noble memory; what of him?\n",
      "Our slaught and grief is now born to repend.\n",
      "\n",
      "JULIET:\n",
      "I do confess it, Tranio, for the face\n",
      "Be merry, gods. Imply that know it.\n",
      "\n",
      "GLOUCESTER:\n",
      "The queen is valued thirty.\n",
      "\n",
      "WARWICK:\n",
      "How now, my heart! how now, what news with you,\n",
      "She hath a poor men of this.\n",
      "\n",
      "JULIET:\n",
      "I will tell thee, Let me be thus bold.\n",
      "\n",
      "LADY ANNE:\n",
      "I would I knew thy heart.\n",
      "\n",
      "GLOUCESTER:\n",
      "\n",
      "CLARENCE:\n",
      "\n",
      "KING EDWARD IV:\n",
      "Nay, this shall be.\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "So smile the heavens and them\n",
      "At hand that I may be: but I cannot\n",
      "Believe this crack to be ingrateful,\n",
      "were to draw in many a tear\n",
      "And stop the rising of blood-sucking but the base court of the\n",
      "two of the house of the Montagues.\n",
      "\n",
      "SAMPSON:\n",
      "My noble lords and cousin Buckingham,\n",
      "Then cursed she Richard. O, remember, God\n",
      "To hear her prayers for the fair Bianca:\n",
      "And thou shalt tell the truth.\n",
      "\n",
      "BRUTUS:\n",
      "Being the red plague rid you\n",
      "For learning me your language!\n",
      "\n",
      "PETRUCHIO:\n",
      "No, couple mouths\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nHamlet's soliloquy: \")\n",
    "print(generate_text(model, start_string=\"HAMLET: To be, or not to be\", temperature=0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creative writing:\n",
      "Once upon a time monur, while mef we entreat him.\n",
      "\n",
      "PETRUCHIO:\n",
      "Fhy, Harry, we sings down toh,\n",
      "Put to the friar,\n",
      "Yet,--ay, er:'\n",
      "Those old deserved it;\n",
      "And therein fears; the bay-lenced\n",
      "Than Bience did for, i'er hide,\n",
      "Which with suspicion!\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "He proseeveth g, ar I dare swear thio! how he's unto revenge,\n",
      "Which ICHARD:\n",
      "Then oncl we speak, of all\n",
      "The charges, of the third a\n",
      "diber hoarser'd: I will tell you 'fore.'\n",
      "\n",
      "WARWIO:\n",
      "My Lord!\n",
      "\n",
      "Ghost of Hereford,?\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "Noth, and yoursband,\n",
      "You will descends.\n",
      "\n",
      "LUCENTIO:\n",
      "Come, come, deny along\n",
      "Of your queen as yours but use her horn-dids death forsaid\n",
      "unto the ear, when\n",
      "But yet nt.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "Thy life resolved.\n",
      "\n",
      "Shepherd:\n",
      "Ay, school-mages, kepe all avishs, yet look where they quarter'd broke a jadnes.\n",
      "\n",
      "MERCUTIO:\n",
      "Confusion!\n",
      "\n",
      "LUCIO:\n",
      "Down, down toRMASER:\n",
      "Beshall yield you, ele?\n",
      "\n",
      "Page:\n",
      "His name, my lord; and\n",
      "love tosh, entertain use tongue in fair proconsA:\n",
      "Not over of these are my ships and great Italy?\n",
      "\n",
      "BAPTISTA:\n",
      "Is it here? 'twas no vel\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCreative writing:\")\n",
    "print(generate_text(model, start_string=\"Once upon a time\", temperature=1.5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
